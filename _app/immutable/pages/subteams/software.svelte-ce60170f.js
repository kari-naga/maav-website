import{S as Z,i as tt,s as et,k as v,x as ot,a5 as st,d as e,m as w,y as at,g as a,z as it,r as nt,p as lt,C as rt,e as f,t as m,c as d,a as h,h as c,b as S,M as u,n as ut}from"../../chunks/index-68b7d76e.js";import{C as mt}from"../../chunks/Content-ff97005f.js";import{b as ct}from"../../chunks/basePath-7a7ff4de.js";function ft(R){let i,b,n,l,s,r,p,W,P,_,M,C,g,D,$,L,T,E,x,B,A,k,H,N,y,O,q,G,U;return{c(){i=f("a"),b=m("Back to Subteams"),n=v(),l=f("h3"),s=m("Perception"),r=v(),p=f("p"),W=m("The Software subteam uses Intel Realsense RGBD cameras and LIDAR to view our surrounding world. To solve our challenges in the new mission, we research and develop state-of-the-art algorithms to perform detection, segmentation, fitting, tracking, and other functions on the image and point cloud. Some problems we are working on include visual odometry, image stitching, object detection, and tracking using advanced neural networks. We also use LIDAR to estimate the position of obstacles. Our algorithms are run on a Jetson TX2 embedded GPU."),P=v(),_=f("h3"),M=m("Guidance, Navigation, and Control"),C=v(),g=f("p"),D=m("A core part of our team's work is in "),$=f("span"),L=m("Guidance, Navigation, and Control (GNC)"),T=m(". GNC feeds inputs from the vision team and our sensors into SLAM algorithms to figure out where the robot is in the world around it, and is also responsible for getting the quadcopter to where it needs to go (as instructed by Mission Planning)."),E=v(),x=f("h3"),B=m("Utilities"),A=v(),k=f("p"),H=m("Software is also responsible for developing tools to aid the rest of the software team. We maintain a Gazebo simulation used to test the rest of our software in a virtual environment by simulating sensor inputs. We also develop a ground control station used to command the quadcopter manually as well as a mobile app that accepts voice commands for the vehicle."),N=v(),y=f("h3"),O=m("Mission Planning"),q=v(),G=f("p"),U=m("Finally, Software develops algorithms to control the behavior of our quadcopter to complete the mission. Our mission strategy algorithm is the brain of our vehicle, which would not be autonomous without it. It involves multi-robot collaboration and human-robot interaction. We work actively with graduate level robotics students to find the best solution for this new challenging mission."),this.h()},l(t){i=d(t,"A",{href:!0,class:!0});var o=h(i);b=c(o,"Back to Subteams"),o.forEach(e),n=w(t),l=d(t,"H3",{class:!0});var j=h(l);s=c(j,"Perception"),j.forEach(e),r=w(t),p=d(t,"P",{});var F=h(p);W=c(F,"The Software subteam uses Intel Realsense RGBD cameras and LIDAR to view our surrounding world. To solve our challenges in the new mission, we research and develop state-of-the-art algorithms to perform detection, segmentation, fitting, tracking, and other functions on the image and point cloud. Some problems we are working on include visual odometry, image stitching, object detection, and tracking using advanced neural networks. We also use LIDAR to estimate the position of obstacles. Our algorithms are run on a Jetson TX2 embedded GPU."),F.forEach(e),P=w(t),_=d(t,"H3",{class:!0});var J=h(_);M=c(J,"Guidance, Navigation, and Control"),J.forEach(e),C=w(t),g=d(t,"P",{});var I=h(g);D=c(I,"A core part of our team's work is in "),$=d(I,"SPAN",{class:!0});var X=h($);L=c(X,"Guidance, Navigation, and Control (GNC)"),X.forEach(e),T=c(I,". GNC feeds inputs from the vision team and our sensors into SLAM algorithms to figure out where the robot is in the world around it, and is also responsible for getting the quadcopter to where it needs to go (as instructed by Mission Planning)."),I.forEach(e),E=w(t),x=d(t,"H3",{class:!0});var K=h(x);B=c(K,"Utilities"),K.forEach(e),A=w(t),k=d(t,"P",{});var Q=h(k);H=c(Q,"Software is also responsible for developing tools to aid the rest of the software team. We maintain a Gazebo simulation used to test the rest of our software in a virtual environment by simulating sensor inputs. We also develop a ground control station used to command the quadcopter manually as well as a mobile app that accepts voice commands for the vehicle."),Q.forEach(e),N=w(t),y=d(t,"H3",{class:!0});var V=h(y);O=c(V,"Mission Planning"),V.forEach(e),q=w(t),G=d(t,"P",{});var Y=h(G);U=c(Y,"Finally, Software develops algorithms to control the behavior of our quadcopter to complete the mission. Our mission strategy algorithm is the brain of our vehicle, which would not be autonomous without it. It involves multi-robot collaboration and human-robot interaction. We work actively with graduate level robotics students to find the best solution for this new challenging mission."),Y.forEach(e),this.h()},h(){S(i,"href",`${ct}/subteams/`),S(i,"class","font-light text-xl hover:text-blue-600 active:text-blue-800"),S(l,"class","text-xl font-medium"),S(_,"class","text-xl font-medium"),S($,"class","font-medium"),S(x,"class","text-xl font-medium"),S(y,"class","text-xl font-medium")},m(t,o){a(t,i,o),u(i,b),a(t,n,o),a(t,l,o),u(l,s),a(t,r,o),a(t,p,o),u(p,W),a(t,P,o),a(t,_,o),u(_,M),a(t,C,o),a(t,g,o),u(g,D),u(g,$),u($,L),u(g,T),a(t,E,o),a(t,x,o),u(x,B),a(t,A,o),a(t,k,o),u(k,H),a(t,N,o),a(t,y,o),u(y,O),a(t,q,o),a(t,G,o),u(G,U)},p:ut,d(t){t&&e(i),t&&e(n),t&&e(l),t&&e(r),t&&e(p),t&&e(P),t&&e(_),t&&e(C),t&&e(g),t&&e(E),t&&e(x),t&&e(A),t&&e(k),t&&e(N),t&&e(y),t&&e(q),t&&e(G)}}}function dt(R){let i,b,n,l;return document.title=i=z,n=new mt({props:{title:z,$$slots:{default:[ft]},$$scope:{ctx:R}}}),{c(){b=v(),ot(n.$$.fragment)},l(s){st('[data-svelte="svelte-1258swp"]',document.head).forEach(e),b=w(s),at(n.$$.fragment,s)},m(s,r){a(s,b,r),it(n,s,r),l=!0},p(s,[r]){(!l||r&0)&&i!==(i=z)&&(document.title=i);const p={};r&1&&(p.$$scope={dirty:r,ctx:s}),n.$set(p)},i(s){l||(nt(n.$$.fragment,s),l=!0)},o(s){lt(n.$$.fragment,s),l=!1},d(s){s&&e(b),rt(n,s)}}}const z="Software";class wt extends Z{constructor(i){super(),tt(this,i,null,dt,et,{})}}export{wt as default};
